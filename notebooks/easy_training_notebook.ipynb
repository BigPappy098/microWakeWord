{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy microWakeWord Training\n",
    "\n",
    "This notebook provides a simplified approach to training custom wake word models using microWakeWord. It's designed to be accessible to users with minimal machine learning experience while still producing high-quality models.\n",
    "\n",
    "This notebook uses the direct training approach from the basic notebook but with more user-friendly explanations and visual elements to help you understand and customize the training process.\n",
    "\n",
    "## What You'll Need\n",
    "\n",
    "- Python 3.10 installed\n",
    "- A GPU is recommended for faster training (but not required)\n",
    "- Your desired wake word phrase (e.g., \"hey computer\")\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install microWakeWord and dependencies\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    # `pymicro-features` is installed from a fork to support building on macOS\n",
    "    !pip install 'git+https://github.com/puddly/pymicro-features@puddly/minimum-cpp-version'\n",
    "\n",
    "# `audio-metadata` is installed from a fork to unpin `attrs` from a version that breaks Jupyter\n",
    "!pip install 'git+https://github.com/whatsnowplaying/audio-metadata@d4ebb238e6a401bb1a5aaaac60c9e2b3cb30929f'\n",
    "\n",
    "# Install ipywidgets for interactive notebook elements\n",
    "!pip install ipywidgets\n",
    "\n",
    "!git clone https://github.com/BigPappy098/microWakeWord\n",
    "!pip install -e ./microWakeWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Choose Your Wake Word\n",
    "\n",
    "Choose a wake word phrase that you want to use. Good wake words typically have:\n",
    "- Multiple syllables (3-5 is ideal)\n",
    "- Distinctive sounds that don't commonly appear in everyday speech\n",
    "- Clear pronunciation\n",
    "\n",
    "Examples: \"hey computer\", \"jarvis\", \"alexa\", \"computer\"\n",
    "\n",
    "You can use phonetic spellings to improve recognition. For example, \"computer\" might be better as \"kuhm-pyoo-ter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your wake word here\n",
    "wake_word = \"hey_computer\"  # Use underscores instead of spaces\n",
    "\n",
    "# Listen to a sample of how it will sound\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import Audio\n",
    "\n",
    "if not os.path.exists(\"./piper-sample-generator\"):\n",
    "    !git clone https://github.com/rhasspy/piper-sample-generator\n",
    "    !wget -O piper-sample-generator/models/en_US-libritts_r-medium.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt'\n",
    "    !pip install torch torchaudio piper-phonemize-cross==1.2.1\n",
    "\n",
    "    if \"piper-sample-generator/\" not in sys.path:\n",
    "        sys.path.append(\"piper-sample-generator/\")\n",
    "\n",
    "!mkdir -p sample_test\n",
    "!python3 piper-sample-generator/generate_samples.py \"{wake_word}\" \\\n",
    "--max-samples 1 \\\n",
    "--batch-size 1 \\\n",
    "--output-dir sample_test\n",
    "\n",
    "Audio(\"sample_test/0.wav\", autoplay=True)\n",
    "\n",
    "# Create directory for generated samples\n",
    "!mkdir -p generated_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Choose Training Parameters\n",
    "\n",
    "Now, let's configure the training process based on your wake word and needs:\n",
    "\n",
    "1. **Wake Word Length**: Choose a preset based on the length of your wake word\n",
    "   - `short`: For 1-2 syllable wake words (e.g., \"jarvis\")\n",
    "   - `medium`: For 3-4 syllable wake words (e.g., \"hey computer\")\n",
    "   - `long`: For 5+ syllable wake words (e.g., \"hey google assistant\")\n",
    "\n",
    "2. **Augmentation Level**: Choose how much to vary the training samples\n",
    "   - `light`: Less variation, good for quiet environments\n",
    "   - `medium`: Balanced variation, good for most home environments\n",
    "   - `heavy`: High variation, good for noisy environments\n",
    "\n",
    "3. **Sample Count**: How many synthetic samples to generate\n",
    "   - 500-1000 is good for testing\n",
    "   - 2000-5000 is recommended for production models\n",
    "   \n",
    "4. **Batch Size**: Size of batches during training\n",
    "   - Larger values may train faster but require more memory\n",
    "   - Smaller values use less memory but may train slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training parameters\n",
    "preset = \"medium\"  # Choose from: \"short\", \"medium\", \"long\"\n",
    "augmentation_level = \"medium\"  # Choose from: \"light\", \"medium\", \"heavy\"\n",
    "samples_count = 1000  # Number of samples to generate\n",
    "batch_size = 128  # Batch size for training (larger values may be faster but require more memory)\n",
    "\n",
    "# Output directory\n",
    "output_dir = f\"trained_models/{wake_word}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Negative Samples\n",
    "\n",
    "To train a robust model, we need \"negative\" samples - audio that is NOT the wake word. These help the model learn what to ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download negative datasets\n",
    "output_dir = './negative_datasets'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    link_root = \"https://huggingface.co/datasets/kahrendt/microwakeword/resolve/main/\"\n",
    "    filenames = ['dinner_party.zip', 'dinner_party_eval.zip', 'no_speech.zip', 'speech.zip']\n",
    "    for fname in filenames:\n",
    "        link = link_root + fname\n",
    "        zip_path = f\"negative_datasets/{fname}\"\n",
    "        !wget -O {zip_path} {link}\n",
    "        !unzip -q {zip_path} -d {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Wake Word Samples\n",
    "\n",
    "Now we'll generate a larger number of wake word samples for training. This step creates synthetic audio samples of your wake word with different voices and variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a larger number of wake word samples for training\n",
    "!python3 piper-sample-generator/generate_samples.py \"{wake_word}\" \\\n",
    "--max-samples {samples_count} \\\n",
    "--batch-size {batch_size} \\\n",
    "--output-dir generated_samples\n",
    "\n",
    "print(f\"Generated {samples_count} samples with batch size {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set Up Augmentation\n",
    "\n",
    "Now we'll set up the audio augmentation pipeline. This adds variations to our samples like background noise, distortion, and room effects to make the model more robust in real-world environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up augmentation based on the selected level\n",
    "from microwakeword.audio.augmentation import Augmentation\n",
    "from microwakeword.audio.clips import Clips\n",
    "from microwakeword.audio.spectrograms import SpectrogramGeneration\n",
    "\n",
    "# Define augmentation parameters based on the selected level\n",
    "augmentation_configs = {\n",
    "    \"light\": {\n",
    "        \"probabilities\": {\n",
    "            \"SevenBandParametricEQ\": 0.05,\n",
    "            \"TanhDistortion\": 0.05,\n",
    "            \"PitchShift\": 0.05,\n",
    "            \"BandStopFilter\": 0.05,\n",
    "            \"AddColorNoise\": 0.05,\n",
    "            \"AddBackgroundNoise\": 0.5,\n",
    "            \"Gain\": 1.0,\n",
    "            \"RIR\": 0.3,\n",
    "        },\n",
    "        \"min_snr\": 0,\n",
    "        \"max_snr\": 15,\n",
    "    },\n",
    "    \"medium\": {\n",
    "        \"probabilities\": {\n",
    "            \"SevenBandParametricEQ\": 0.1,\n",
    "            \"TanhDistortion\": 0.1,\n",
    "            \"PitchShift\": 0.1,\n",
    "            \"BandStopFilter\": 0.1,\n",
    "            \"AddColorNoise\": 0.1,\n",
    "            \"AddBackgroundNoise\": 0.75,\n",
    "            \"Gain\": 1.0,\n",
    "            \"RIR\": 0.5,\n",
    "        },\n",
    "        \"min_snr\": -3,\n",
    "        \"max_snr\": 12,\n",
    "    },\n",
    "    \"heavy\": {\n",
    "        \"probabilities\": {\n",
    "            \"SevenBandParametricEQ\": 0.3,\n",
    "            \"TanhDistortion\": 0.2,\n",
    "            \"PitchShift\": 0.3,\n",
    "            \"BandStopFilter\": 0.2,\n",
    "            \"AddColorNoise\": 0.3,\n",
    "            \"AddBackgroundNoise\": 0.9,\n",
    "            \"Gain\": 1.0,\n",
    "            \"RIR\": 0.7,\n",
    "        },\n",
    "        \"min_snr\": -5,\n",
    "        \"max_snr\": 10,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Get the selected augmentation configuration\n",
    "aug_config = augmentation_configs[augmentation_level]\n",
    "\n",
    "# Set up the clips and augmentation\n",
    "clips = Clips(\n",
    "    input_directory='generated_samples',\n",
    "    file_pattern='*.wav',\n",
    "    max_clip_duration_s=None,\n",
    "    remove_silence=False,\n",
    "    random_split_seed=10,\n",
    "    split_count=0.1,\n",
    ")\n",
    "\n",
    "augmenter = Augmentation(\n",
    "    augmentation_duration_s=3.2,\n",
    "    augmentation_probabilities=aug_config[\"probabilities\"],\n",
    "    impulse_paths=['mit_rirs'] if os.path.exists('mit_rirs') else [],\n",
    "    background_paths=['fma_16k', 'audioset_16k'] if os.path.exists('fma_16k') else [],\n",
    "    background_min_snr_db=aug_config[\"min_snr\"],\n",
    "    background_max_snr_db=aug_config[\"max_snr\"],\n",
    "    min_jitter_s=0.195,\n",
    "    max_jitter_s=0.205,\n",
    ")\n",
    "\n",
    "print(f\"Augmentation set up with {augmentation_level} level\")\n",
    "print(f\"Background noise SNR range: {aug_config['min_snr']} to {aug_config['max_snr']} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Spectrograms\n",
    "\n",
    "Now we'll generate spectrograms from our audio samples. Spectrograms are visual representations of the audio that the neural network will learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for spectrograms\n",
    "!mkdir -p generated_augmented_features\n",
    "\n",
    "# Create spectrogram generator\n",
    "spectrograms = SpectrogramGeneration(\n",
    "    clips=clips,\n",
    "    augmenter=augmenter,\n",
    "    slide_frames=10,  # Uses the same spectrogram repeatedly, just shifted over by one frame\n",
    "    step_ms=10,\n",
    ")\n",
    "\n",
    "# Generate spectrograms for training\n",
    "print(\"Generating spectrograms for training... This may take a while.\")\n",
    "from microwakeword.audio.ragged_mmap import RaggedMmap\n",
    "\n",
    "# Generate training spectrograms\n",
    "RaggedMmap.from_generator(\n",
    "    out_dir=os.path.join('generated_augmented_features', 'wakeword_mmap'),\n",
    "    sample_generator=spectrograms.spectrogram_generator(split=\"train\", repeat=2),\n",
    "    batch_size=batch_size,  # Using the same batch size as training\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Generate validation spectrograms\n",
    "validation_spectrograms = SpectrogramGeneration(\n",
    "    clips=clips,\n",
    "    augmenter=augmenter,\n",
    "    slide_frames=10,\n",
    "    step_ms=10,\n",
    ")\n",
    "\n",
    "RaggedMmap.from_generator(\n",
    "    out_dir=os.path.join('generated_augmented_features', 'validation'),\n",
    "    sample_generator=validation_spectrograms.spectrogram_generator(split=\"validation\", repeat=1),\n",
    "    batch_size=batch_size,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Generate testing spectrograms\n",
    "testing_spectrograms = SpectrogramGeneration(\n",
    "    clips=clips,\n",
    "    augmenter=augmenter,\n",
    "    slide_frames=1,  # Use slide_frames=1 for testing to simulate streaming\n",
    "    step_ms=10,\n",
    ")\n",
    "\n",
    "RaggedMmap.from_generator(\n",
    "    out_dir=os.path.join('generated_augmented_features', 'testing'),\n",
    "    sample_generator=testing_spectrograms.spectrogram_generator(split=\"test\", repeat=1),\n",
    "    batch_size=batch_size,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"All spectrograms generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Training Configuration\n",
    "\n",
    "Now we'll create a configuration file that controls the training process. This includes model architecture, training parameters, and data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Define model presets based on wake word length\n",
    "model_presets = {\n",
    "    \"short\": {  # For short wake words (1-2 syllables)\n",
    "        \"pointwise_filters\": \"48,48,48,48\",\n",
    "        \"repeat_in_block\": \"1,1,1,1\",\n",
    "        \"mixconv_kernel_sizes\": \"[5],[7,11],[9,15],[17]\",\n",
    "        \"residual_connection\": \"0,0,0,0\",\n",
    "        \"first_conv_filters\": 32,\n",
    "        \"first_conv_kernel_size\": 5,\n",
    "        \"stride\": 3,\n",
    "        \"training_steps\": 15000,\n",
    "        \"negative_class_weight\": 15,\n",
    "    },\n",
    "    \"medium\": {  # For medium wake words (3-4 syllables)\n",
    "        \"pointwise_filters\": \"64,64,64,64\",\n",
    "        \"repeat_in_block\": \"1,1,1,1\",\n",
    "        \"mixconv_kernel_sizes\": \"[5],[7,11],[9,15],[23]\",\n",
    "        \"residual_connection\": \"0,0,0,0\",\n",
    "        \"first_conv_filters\": 32,\n",
    "        \"first_conv_kernel_size\": 5,\n",
    "        \"stride\": 3,\n",
    "        \"training_steps\": 20000,\n",
    "        \"negative_class_weight\": 20,\n",
    "    },\n",
    "    \"long\": {  # For longer wake words (5+ syllables)\n",
    "        \"pointwise_filters\": \"64,64,64,64\",\n",
    "        \"repeat_in_block\": \"1,1,1,1\",\n",
    "        \"mixconv_kernel_sizes\": \"[5],[7,11],[9,15],[29]\",\n",
    "        \"residual_connection\": \"0,0,0,0\",\n",
    "        \"first_conv_filters\": 32,\n",
    "        \"first_conv_kernel_size\": 5,\n",
    "        \"stride\": 3,\n",
    "        \"training_steps\": 25000,\n",
    "        \"negative_class_weight\": 25,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Get the selected model preset\n",
    "selected_preset = model_presets[preset]\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(f\"trained_models/{wake_word}/model\", exist_ok=True)\n",
    "\n",
    "# Create training configuration\n",
    "config = {}\n",
    "config[\"window_step_ms\"] = 10\n",
    "config[\"train_dir\"] = f\"trained_models/{wake_word}/model\"\n",
    "config[\"summaries_dir\"] = os.path.join(config[\"train_dir\"], \"summaries\")\n",
    "\n",
    "# Define feature sources\n",
    "config[\"features\"] = [\n",
    "    {\n",
    "        \"features_dir\": \"generated_augmented_features\",\n",
    "        \"sampling_weight\": 2.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": True,\n",
    "        \"truncation_strategy\": \"truncate_start\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        \"features_dir\": \"negative_datasets/speech\",\n",
    "        \"sampling_weight\": 10.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"random\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        \"features_dir\": \"negative_datasets/dinner_party\",\n",
    "        \"sampling_weight\": 10.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"random\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    {\n",
    "        \"features_dir\": \"negative_datasets/no_speech\",\n",
    "        \"sampling_weight\": 5.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"random\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "    { # Only used for validation and testing\n",
    "        \"features_dir\": \"negative_datasets/dinner_party_eval\",\n",
    "        \"sampling_weight\": 0.0,\n",
    "        \"penalty_weight\": 1.0,\n",
    "        \"truth\": False,\n",
    "        \"truncation_strategy\": \"split\",\n",
    "        \"type\": \"mmap\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "config[\"training_steps\"] = [selected_preset[\"training_steps\"]]\n",
    "config[\"positive_class_weight\"] = [1]\n",
    "config[\"negative_class_weight\"] = [selected_preset[\"negative_class_weight\"]]\n",
    "config[\"learning_rates\"] = [0.001]\n",
    "config[\"batch_size\"] = batch_size\n",
    "config[\"time_mask_max_size\"] = [5]\n",
    "config[\"time_mask_count\"] = [2]\n",
    "config[\"freq_mask_max_size\"] = [5]\n",
    "config[\"freq_mask_count\"] = [2]\n",
    "config[\"eval_step_interval\"] = 500\n",
    "config[\"clip_duration_ms\"] = 1500\n",
    "config[\"target_minimization\"] = 0.9\n",
    "config[\"maximization_metric\"] = \"average_viable_recall\"\n",
    "\n",
    "# Get a sample spectrogram to determine dimensions\n",
    "sample_spec = spectrograms.get_random_spectrogram()\n",
    "config[\"spectrogram_length\"] = sample_spec.shape[0]\n",
    "config[\"feature_count\"] = sample_spec.shape[1]\n",
    "config[\"training_input_shape\"] = [config[\"spectrogram_length\"], config[\"feature_count\"]]\n",
    "\n",
    "print(f\"Spectrogram dimensions: {config['spectrogram_length']} x {config['feature_count']}\")\n",
    "\n",
    "# Save configuration to file\n",
    "config_path = \"training_parameters.yaml\"\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config, file)\n",
    "\n",
    "print(f\"Training configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train the Model\n",
    "\n",
    "Now we'll train the neural network model using the configuration we created. This process will:\n",
    "1. Train the model on the spectrograms we generated\n",
    "2. Convert the model to a streaming TFLite format for deployment\n",
    "3. Test the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the direct approach from the basic notebook\n",
    "# This gives us more control and consistency in the training process\n",
    "\n",
    "# Get model parameters from the selected preset\n",
    "pointwise_filters = selected_preset[\"pointwise_filters\"]\n",
    "repeat_in_block = selected_preset[\"repeat_in_block\"]\n",
    "mixconv_kernel_sizes = selected_preset[\"mixconv_kernel_sizes\"]\n",
    "residual_connection = selected_preset[\"residual_connection\"]\n",
    "first_conv_filters = selected_preset[\"first_conv_filters\"]\n",
    "first_conv_kernel_size = selected_preset[\"first_conv_kernel_size\"]\n",
    "stride = selected_preset[\"stride\"]\n",
    "\n",
    "# Run the training command\n",
    "!python -m microwakeword.model_train_eval \\\n",
    "--training_config='training_parameters.yaml' \\\n",
    "--train 1 \\\n",
    "--restore_checkpoint 1 \\\n",
    "--test_tf_nonstreaming 0 \\\n",
    "--test_tflite_nonstreaming 0 \\\n",
    "--test_tflite_nonstreaming_quantized 0 \\\n",
    "--test_tflite_streaming 0 \\\n",
    "--test_tflite_streaming_quantized 1 \\\n",
    "--use_weights \"best_weights\" \\\n",
    "mixednet \\\n",
    "--pointwise_filters \"{pointwise_filters}\" \\\n",
    "--repeat_in_block  \"{repeat_in_block}\" \\\n",
    "--mixconv_kernel_sizes '{mixconv_kernel_sizes}' \\\n",
    "--residual_connection \"{residual_connection}\" \\\n",
    "--first_conv_filters {first_conv_filters} \\\n",
    "--first_conv_kernel_size {first_conv_kernel_size} \\\n",
    "--stride {stride}\n",
    "\n",
    "print(f\"Training completed! Model saved to trained_models/{wake_word}/model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Download Your Model\n",
    "\n",
    "Once training is complete, you can download your model for use with ESPHome or other compatible systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Path to the trained model\n",
    "model_file = os.path.join(f\"trained_models/{wake_word}/model/tflite_stream_state_internal_quant/stream_state_internal_quant.tflite\")\n",
    "\n",
    "if os.path.exists(model_file):\n",
    "    print(f\"Your model is ready! Click below to download:\")\n",
    "    display(FileLink(model_file))\n",
    "else:\n",
    "    print(f\"Model file not found at {model_file}. Check for errors in the training process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Create a Model Manifest for ESPHome\n",
    "\n",
    "To use your model with ESPHome, you need to create a model manifest JSON file. Here's a template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create a model manifest for ESPHome\n",
    "manifest = {\n",
    "    \"name\": wake_word,\n",
    "    \"version\": 2,\n",
    "    \"type\": \"micro_speech\",\n",
    "    \"description\": f\"Custom wake word model for '{wake_word}'\",\n",
    "    \"specs\": {\n",
    "        \"average_window_length\": 10,\n",
    "        \"detection_threshold\": 0.7,\n",
    "        \"suppression_ms\": 1000,\n",
    "        \"minimum_count\": 3,\n",
    "        \"sample_rate\": 16000,\n",
    "        \"vocabulary\": [\"_silence_\", \"_unknown_\", wake_word]\n",
    "    }\n",
    "}\n",
    "\n",
    "manifest_file = os.path.join(f\"trained_models/{wake_word}/model/manifest.json\")\n",
    "with open(manifest_file, 'w') as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"Model manifest created at {manifest_file}\")\n",
    "display(FileLink(manifest_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting and Fine-Tuning\n",
    "\n",
    "If your model doesn't perform as expected, here are some tips:\n",
    "\n",
    "1. **False Positives** (activates too often):\n",
    "   - Increase the `negative_class_weight` in the training configuration\n",
    "   - Increase the `detection_threshold` in the manifest file\n",
    "   - Try a different phonetic spelling of your wake word\n",
    "\n",
    "2. **False Negatives** (doesn't activate when it should):\n",
    "   - Decrease the `negative_class_weight` in the training configuration\n",
    "   - Decrease the `detection_threshold` in the manifest file\n",
    "   - Generate more training samples\n",
    "   - Try a different phonetic spelling of your wake word\n",
    "\n",
    "3. **Advanced Configuration**:\n",
    "   - You can modify the training_parameters.yaml file directly for more control\n",
    "   - Increase training_steps for better accuracy (but longer training time)\n",
    "   - Adjust augmentation parameters for different environments\n",
    "   - Try different model architectures by changing the mixednet parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
